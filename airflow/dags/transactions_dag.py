"""
DAG principal para procesamiento de transacciones
Ejercicio 1: Orquestaci√≥n local end-to-end
"""

from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
from airflow.sensors.filesystem import FileSensor
from airflow.utils.dates import days_ago
import pandas as pd
import sqlite3
from pathlib import Path
import logging

# Configuraci√≥n del DAG
default_args = {
    'owner': 'data-engineering-team',
    'depends_on_past': False,
    'start_date': days_ago(1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 3,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    'transactions_etl_pipeline',
    default_args=default_args,
    description='Pipeline ETL para transacciones - Ejercicio 1',
    schedule_interval=timedelta(hours=1),
    catchup=False,
    tags=['etl', 'transactions', 'ejercicio1'],
)

# Configuraci√≥n de rutas
DATA_DIR = Path('/opt/airflow/data')
RAW_DIR = DATA_DIR / 'raw'
PROCESSED_DIR = DATA_DIR / 'processed'
DB_PATH = PROCESSED_DIR / 'transactions.db'

def check_file_size(**context):
    """
    Sensor personalizado: verificar que el archivo tenga tama√±o m√≠nimo
    Requisito: sensores (espera de archivo, tama√±o m√≠nimo)
    """
    file_path = RAW_DIR / 'sample_transactions.csv'
    
    if not file_path.exists():
        raise FileNotFoundError(f"Archivo no encontrado: {file_path}")
    
    file_size = file_path.stat().st_size
    min_size = 1024 * 1024  # 1MB m√≠nimo
    
    if file_size < min_size:
        raise ValueError(f"Archivo muy peque√±o: {file_size} bytes < {min_size} bytes")
    
    logging.info(f"‚úÖ Archivo v√°lido: {file_size} bytes")
    return file_path

def extract_data(**context):
    """
    Extrae datos del CSV con lectura por chunks
    Requisitos: lectura por chunks para eficiencia de memoria
    """
    file_path = context['task_instance'].xcom_pull(task_ids='check_file_size')
    
    logging.info(f"üì• Extrayendo datos de: {file_path}")
    
    # Lectura por chunks para archivos grandes
    chunk_size = 10000
    chunks = []
    total_rows = 0
    
    try:
        for chunk in pd.read_csv(file_path, chunksize=chunk_size):
            chunks.append(chunk)
            total_rows += len(chunk)
            
            if total_rows % 100000 == 0:
                logging.info(f"üìä Procesadas {total_rows} filas...")
        
        df = pd.concat(chunks, ignore_index=True)
        
        # M√©tricas de extracci√≥n
        metrics = {
            'rows_extracted': len(df),
            'columns': list(df.columns),
            'file_size_mb': file_path.stat().st_size / (1024*1024),
            'extraction_time': datetime.now().isoformat()
        }
        
        logging.info(f"‚úÖ Extracci√≥n completada: {metrics}")
        
        # Guardar datos temporalmente
        temp_path = PROCESSED_DIR / 'temp_extracted.csv'
        df.to_csv(temp_path, index=False)
        
        return {
            'temp_file': str(temp_path),
            'metrics': metrics
        }
        
    except Exception as e:
        logging.error(f"‚ùå Error en extracci√≥n: {e}")
        raise

def transform_data(**context):
    """
    Transforma y limpia los datos
    Requisitos: transforme datos con Python
    """
    extract_result = context['task_instance'].xcom_pull(task_ids='extract_data')
    temp_file = extract_result['temp_file']
    
    logging.info(f"üîÑ Transformando datos de: {temp_file}")
    
    try:
        df = pd.read_csv(temp_file)
        
        # Transformaciones
        initial_count = len(df)
        
        # Limpiar datos nulos
        df = df.dropna()
        
        # Validar tipos de datos
        if 'amount' in df.columns:
            df = df[df['amount'] > 0]
        
        if 'user_id' in df.columns:
            df = df[df['user_id'].notna()]
        
        # Normalizar timestamp
        if 'timestamp' in df.columns:
            df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')
            df = df[df['timestamp'].notna()]
        
        # Agregar columnas calculadas
        df['processed_at'] = datetime.now()
        df['data_quality_score'] = 1.0  # Simplificado
        
        final_count = len(df)
        
        # M√©tricas de transformaci√≥n
        metrics = {
            'rows_input': initial_count,
            'rows_output': final_count,
            'data_quality_ratio': final_count / initial_count if initial_count > 0 else 0,
            'transformation_time': datetime.now().isoformat()
        }
        
        logging.info(f"‚úÖ Transformaci√≥n completada: {metrics}")
        
        # Guardar datos transformados
        transformed_path = PROCESSED_DIR / 'transformed_transactions.csv'
        df.to_csv(transformed_path, index=False)
        
        return {
            'transformed_file': str(transformed_path),
            'metrics': metrics
        }
        
    except Exception as e:
        logging.error(f"‚ùå Error en transformaci√≥n: {e}")
        raise

def load_to_database(**context):
    """
    Carga datos a SQLite/PostgreSQL
    Requisitos: cargue a SQLite/PostgreSQL
    """
    transform_result = context['task_instance'].xcom_pull(task_ids='transform_data')
    transformed_file = transform_result['transformed_file']
    
    logging.info(f"üíæ Cargando datos a base: {DB_PATH}")
    
    try:
        df = pd.read_csv(transformed_file)
        
        # Crear directorio si no existe
        DB_PATH.parent.mkdir(parents=True, exist_ok=True)
        
        # Cargar a SQLite
        with sqlite3.connect(DB_PATH) as conn:
            df.to_sql('transactions', conn, if_exists='replace', index=False)
            
            # Verificar carga
            cursor = conn.execute("SELECT COUNT(*) FROM transactions")
            row_count = cursor.fetchone()[0]
            
            if row_count == 0:
                raise ValueError("‚ùå Tabla destino est√° vac√≠a - ALERTA GENERADA")
            
            # M√©tricas de carga
            metrics = {
                'rows_loaded': row_count,
                'table_name': 'transactions',
                'database_size_mb': DB_PATH.stat().st_size / (1024*1024) if DB_PATH.exists() else 0,
                'load_time': datetime.now().isoformat()
            }
            
            logging.info(f"‚úÖ Carga completada: {metrics}")
            
            return metrics
            
    except Exception as e:
        logging.error(f"‚ùå Error en carga: {e}")
        # Generar alerta simulada
        logging.error("üö® ALERTA: Error en pipeline - notificaci√≥n enviada")
        raise

def validate_data_quality(**context):
    """
    Validar que la tabla destino no est√© vac√≠a
    Requisitos: Validar que la tabla destino no est√© vac√≠a; si lo est√° o hay error, generar alerta
    """
    logging.info(f"üîç Validando calidad de datos en: {DB_PATH}")
    
    try:
        with sqlite3.connect(DB_PATH) as conn:
            # Verificar que la tabla existe y tiene datos
            cursor = conn.execute("SELECT COUNT(*) FROM transactions")
            row_count = cursor.fetchone()[0]
            
            if row_count == 0:
                raise ValueError("‚ùå VALIDACI√ìN FALLIDA: Tabla vac√≠a")
            
            # Validaciones adicionales
            cursor = conn.execute("""
                SELECT 
                    COUNT(*) as total_rows,
                    COUNT(DISTINCT user_id) as unique_users,
                    MIN(amount) as min_amount,
                    MAX(amount) as max_amount,
                    COUNT(CASE WHEN amount <= 0 THEN 1 END) as invalid_amounts
                FROM transactions
            """)
            
            stats = cursor.fetchone()
            
            validation_result = {
                'total_rows': stats[0],
                'unique_users': stats[1], 
                'min_amount': stats[2],
                'max_amount': stats[3],
                'invalid_amounts': stats[4],
                'validation_time': datetime.now().isoformat(),
                'status': 'PASSED'
            }
            
            # Verificar reglas de negocio
            if stats[4] > 0:  # Montos inv√°lidos
                validation_result['status'] = 'WARNING'
                logging.warning(f"‚ö†Ô∏è  Encontrados {stats[4]} montos inv√°lidos")
            
            if stats[1] < 10:  # Muy pocos usuarios √∫nicos
                validation_result['status'] = 'WARNING'
                logging.warning(f"‚ö†Ô∏è  Solo {stats[1]} usuarios √∫nicos encontrados")
            
            logging.info(f"‚úÖ Validaci√≥n completada: {validation_result}")
            
            return validation_result
            
    except Exception as e:
        logging.error(f"‚ùå Error en validaci√≥n: {e}")
        # Generar alerta (simulada)
        logging.error("üö® ALERTA: Validaci√≥n fallida - notificaci√≥n enviada")
        raise

# Definir tareas del DAG

# Sensor de archivo
file_sensor = FileSensor(
    task_id='wait_for_file',
    filepath=str(RAW_DIR / 'sample_transactions.csv'),
    fs_conn_id='fs_default',
    poke_interval=30,
    timeout=300,
    dag=dag,
)

# Verificar tama√±o de archivo
check_size = PythonOperator(
    task_id='check_file_size',
    python_callable=check_file_size,
    dag=dag,
)

# Extracci√≥n
extract = PythonOperator(
    task_id='extract_data',
    python_callable=extract_data,
    dag=dag,
)

# Transformaci√≥n
transform = PythonOperator(
    task_id='transform_data',
    python_callable=transform_data,
    dag=dag,
)

# Carga
load = PythonOperator(
    task_id='load_to_database',
    python_callable=load_to_database,
    dag=dag,
)

# Validaci√≥n
validate = PythonOperator(
    task_id='validate_data_quality',
    python_callable=validate_data_quality,
    dag=dag,
)

# Cleanup temporal
cleanup = BashOperator(
    task_id='cleanup_temp_files',
    bash_command=f'rm -f {PROCESSED_DIR}/temp_*.csv',
    dag=dag,
)

# Definir dependencias
file_sensor >> check_size >> extract >> transform >> load >> validate >> cleanup
