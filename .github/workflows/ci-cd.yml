name: CI/CD Pipeline - Data Engineering

on:
  push:
    branches: [ main, dev, feature/* ]
  pull_request:
    branches: [ main ]

env:
  PYTHON_VERSION: "3.11"
  POETRY_VERSION: "1.7.1"

jobs:
  lint:
    name: Code Quality and Linting
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install black flake8 mypy bandit isort
        pip install pandas sqlite3 pytest
    
    - name: Run Black formatter check
      run: |
        black --check --diff .
      continue-on-error: true
    
    - name: Run isort import sorting
      run: |
        isort --check-only --diff .
      continue-on-error: true
    
    - name: Run Flake8 linter
      run: |
        flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics
        flake8 . --count --exit-zero --max-complexity=10 --max-line-length=88 --statistics
    
    - name: Run MyPy type checker
      run: |
        mypy etl/ modeling/ sql/ --ignore-missing-imports || true
    
    - name: Run Bandit security scanner
      run: |
        bandit -r etl/ modeling/ sql/ -f json -o bandit-report.json || true
    
    - name: Upload security scan results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: bandit-report
        path: bandit-report.json

  test:
    name: Run Tests
    runs-on: ubuntu-latest
    needs: lint
    
    strategy:
      matrix:
        python-version: ["3.10", "3.11", "3.12"]
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y sqlite3
    
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pytest pytest-cov pytest-xdist
        pip install pandas sqlite3
    
    - name: Create test data
      run: |
        mkdir -p data/raw data/processed
        python -c "
import pandas as pd
import random
data = []
for i in range(1000):
    data.append({
        'order_id': i + 1,
        'user_id': random.randint(1, 100),
        'amount': round(random.uniform(10, 1000), 2),
        'status': random.choice(['completed', 'failed', 'pending']),
        'timestamp': f'2025-01-{random.randint(1, 28):02d} {random.randint(0, 23):02d}:00:00'
    })
df = pd.DataFrame(data)
df.to_csv('data/raw/sample_transactions.csv', index=False)
print(f'Generated {len(df)} test records')
        "
    
    - name: Run unit tests
      run: |
        pytest tests/ -v --cov=etl --cov=modeling --cov=sql --cov-report=xml --cov-report=html
    
    - name: Test ETL Pipeline
      run: |
        python main.py etl
    
    - name: Test Warehouse
      run: |
        python main.py warehouse
    
    - name: Test Full Pipeline
      run: |
        python main.py pipeline
    
    - name: Test SQL Analysis
      run: |
        python sql/analysis_runner.py
    
    - name: Upload coverage reports
      uses: codecov/codecov-action@v3
      if: matrix.python-version == '3.11'
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-umbrella

  build:
    name: Build Package
    runs-on: ubuntu-latest
    needs: test
    if: github.ref == 'refs/heads/main'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install build dependencies
      run: |
        python -m pip install --upgrade pip
        pip install build wheel setuptools
    
    - name: Build Python package
      run: |
        python -m build
    
    - name: Check package
      run: |
        pip install twine
        twine check dist/* || true
    
    - name: Upload build artifacts
      uses: actions/upload-artifact@v3
      with:
        name: dist-packages
        path: dist/

  docker:
    name: Build Docker Image
    runs-on: ubuntu-latest
    needs: test
    if: github.ref == 'refs/heads/main'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3
    
    - name: Create Dockerfile
      run: |
        cat > Dockerfile << 'EOF'
        FROM python:3.11-slim
        
        WORKDIR /app
        
        # Install system dependencies
        RUN apt-get update && apt-get install -y \
            sqlite3 \
            && rm -rf /var/lib/apt/lists/*
        
        # Copy requirements
        COPY requirements.txt .
        RUN pip install --no-cache-dir -r requirements.txt
        
        # Copy application code
        COPY . .
        
        # Create data directories
        RUN mkdir -p data/raw data/processed data/warehouse
        
        # Set permissions
        RUN chmod +x main.py
        
        # Default command
        CMD ["python", "main.py", "pipeline"]
        EOF
    
    - name: Build Docker image
      run: |
        docker build -t data-engineering-pipeline:latest .
    
    - name: Test Docker image
      run: |
        docker run --rm data-engineering-pipeline:latest python --version

  integration:
    name: Integration Tests
    runs-on: ubuntu-latest
    needs: [test, build]
    if: github.ref == 'refs/heads/main'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pandas sqlite3 pytest
    
    - name: Generate large test dataset
      run: |
        mkdir -p data/raw
        python -c "
import pandas as pd
import random
import gzip
import json

# Generate 1M transactions for stress test
print('Generating 1M transaction records...')
data = []
for i in range(1000000):
    data.append({
        'order_id': i + 1,
        'user_id': random.randint(1, 10000),
        'amount': round(random.uniform(1, 10000), 2),
        'status': random.choice(['completed', 'failed', 'pending']),
        'timestamp': f'2025-01-{random.randint(1, 28):02d} {random.randint(0, 23):02d}:{random.randint(0, 59):02d}:{random.randint(0, 59):02d}'
    })

df = pd.DataFrame(data)
df.to_csv('data/raw/sample_transactions.csv', index=False)

# Generate log file
print('Generating sample log file...')
log_entries = []
for i in range(100000):
    log_entries.append({
        'timestamp': f'2025-01-{random.randint(1, 28):02d}T{random.randint(0, 23):02d}:{random.randint(0, 59):02d}:{random.randint(0, 59):02d}Z',
        'level': random.choice(['INFO', 'WARN', 'ERROR']),
        'endpoint': random.choice(['/api/transactions', '/api/users', '/api/reports']),
        'status_code': random.choice([200, 400, 500, 502, 503]),
        'response_time': random.uniform(10, 1000)
    })

with gzip.open('data/raw/sample.log.gz', 'wt') as f:
    for entry in log_entries:
        f.write(json.dumps(entry) + '\n')

print(f'Generated {len(df)} transactions and {len(log_entries)} log entries')
        "
    
    - name: Run stress test - ETL 1M records
      run: |
        time python main.py etl
    
    - name: Run stress test - Full pipeline
      run: |
        time python main.py pipeline
    
    - name: Verify large dataset processing
      run: |
        python -c "
import sqlite3
from pathlib import Path

db_path = Path('data/processed/transactions.db')
if db_path.exists():
    with sqlite3.connect(db_path) as conn:
        cursor = conn.execute('SELECT COUNT(*) FROM transactions')
        count = cursor.fetchone()[0]
        print(f'✅ Successfully processed {count:,} records')
        
        if count >= 1000000:
            print('✅ Stress test PASSED - 1M+ records processed')
        else:
            print(f'⚠️  Warning: Only {count} records processed')
else:
    print('❌ Database not found')
        "

  deploy:
    name: Deploy to Staging
    runs-on: ubuntu-latest
    needs: [build, docker, integration]
    if: github.ref == 'refs/heads/main'
    environment: staging
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Simulate deployment
      run: |
        echo "🚀 Deploying to staging environment..."
        echo "📦 Package version: $(date +%Y%m%d-%H%M%S)"
        echo "🐳 Docker image: data-engineering-pipeline:latest"
        echo "📊 Health checks would run here..."
        echo "✅ Deployment completed successfully"
    
    - name: Run smoke tests
      run: |
        echo "🧪 Running smoke tests..."
        python --version
        echo "✅ Smoke tests passed"

  release:
    name: Create Release
    runs-on: ubuntu-latest
    needs: deploy
    if: github.ref == 'refs/heads/main' && startsWith(github.ref, 'refs/tags/')
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Download build artifacts
      uses: actions/download-artifact@v3
      with:
        name: dist-packages
        path: dist/
    
    - name: Create Release
      uses: softprops/action-gh-release@v1
      with:
        files: dist/*
        generate_release_notes: true
        body: |
          ## Data Engineering Pipeline Release
          
          ### Components Included:
          - ✅ ETL Pipeline (Ejercicio 3)
          - ✅ Data Warehouse (Ejercicio 4) 
          - ✅ SQL Analysis (Ejercicio 2)
          - ✅ Airflow Orchestration (Ejercicio 1)
          - ✅ CI/CD Pipeline (Ejercicio 5)
          
          ### Performance:
          - Processes 1M+ records
          - Full pipeline execution
          - Docker containerized
          - Comprehensive testing
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
